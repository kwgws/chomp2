{"cells":[{"cell_type":"markdown","metadata":{},"source":["# CHOMP v2\n","__File Processor__\n","\n","__by Sean Gilleran__  \n","__Last updated November 30__, __2021__  \n","[https://github.com/seangilleran/chomp2](https://github.com/seangilleran/chomp2)\n","\n","This notebook processes raw text files into data more suitable to topic modelling.\n","1. Tokenize the text into individual words.\n","2. Remove stopwords.\n","3. Replace lemmas with a common equivalent where possible.\n","4. Save the result to a new file."]},{"cell_type":"markdown","metadata":{},"source":["## 1. Set Paths"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["in_path = \"./downloads\"\n","out_path = \"./corpus\"\n","\n","stop_words_file = \"stopwords.txt\""]},{"cell_type":"markdown","metadata":{},"source":["## 2. Load"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","\n","files = []\n","total_count = 0\n","skip_count = 0\n","\n","for file in [f for f in os.listdir(in_path) if f.endswith(f\".txt\")]:\n","\n","    if os.path.exists(os.path.join(out_path, file)):\n","        skip_count = skip_count + 1\n","        continue\n","\n","    files.append(file)\n","    total_count = total_count + 1\n","\n","if not os.path.exists(os.path.abspath(out_path)):\n","    os.makedirs(os.path.abspath(out_path))\n","\n","print(f\"Found {total_count} files to process ({skip_count} skipped).\")"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Process\n","\n","This can take a very long time, especially with large data sets! We'll print out a message before each file with a note as to how far we've gotten."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from autocorrect import Speller\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk.tag import pos_tag\n","from nltk.tokenize import word_tokenize\n","import regex as re\n","from unidecode import unidecode\n","\n","nltk.download(\"averaged_perceptron_tagger\")\n","nltk.download(\"punkt\")\n","nltk.download(\"stopwords\")\n","nltk.download(\"wordnet\")\n","nltk.download(\"universal_tagset\")\n","wnl = WordNetLemmatizer()\n","\n","stop_words = set(stopwords.words(\"english\"))\n","with open(stop_words_file, \"r\", encoding=\"utf-8\") as f:\n","    stop_words.update([w.strip() for w in f.readlines()])\n","\n","dehyphenator = re.compile(r\"(?<=[A-Za-z])-\\s\\n(?=[A-Za-z])\")\n","defuzzer = re.compile(r\"([^a-zA-Z0-9]+)\")\n","\n","spell = Speller(fast=True, only_replacements=True)\n","\n","i = 0\n","for file in files:\n","\n","    i = i + 1\n","    print(f\"{i}/{total_count} ({((i / total_count)*100.0):.0f}%): {file}\")\n","\n","    # Load file, remove special characters.\n","    text = \"\"\n","    with open(os.path.join(in_path, file), \"r\", encoding=\"utf-8\") as f:\n","        text = unidecode(f.read())\n","\n","    # De-fuzz.\n","    text = dehyphenator.sub(\"\", text)\n","    text = defuzzer.sub(\" \", text)\n","\n","    # Tokenize.\n","    tokenized_text = word_tokenize(text)\n","    text = []\n","\n","    # Remove stopwords.\n","    for word in [w for w in tokenized_text if w not in stop_words]:\n","        text.append(word)\n","\n","    # Lemmatize.\n","    text = pos_tag(text, tagset=\"universal\")\n","    for x in range(len(text)):\n","        word, pos = text[x]\n","        if pos == \"VERB\":\n","            pos = \"v\"\n","        elif pos == \"ADJ\":\n","            pos = \"a\"\n","        elif pos == \"ADV\":\n","            pos = \"r\"\n","        else:\n","            pos = \"n\"\n","        text[x] = wnl.lemmatize(word, pos=pos)\n","\n","    # One last check to standardize spelling.\n","    text = \" \".join(text)\n","    text = spell(text)\n","\n","    # Save updated text to new file.\n","    with open(os.path.join(out_path, file), \"w\") as f:\n","        f.write(text)\n","\n","print(\"\\n** DONE! **\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":2}

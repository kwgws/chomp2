{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHOMP v2\n",
    "__Internet Archive Collector__\n",
    "\n",
    "__by Sean Gilleran__  \n",
    "__Last updated November 30__, __2021__  \n",
    "[https://github.com/seangilleran/chomp2](https://github.com/seangilleran/chomp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "These cells must be run before any other cells in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Set Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections_file = \"collections.txt\"\n",
    "metadata_path = \"./meta\"\n",
    "download_path = \"./downloads\"\n",
    "\n",
    "search_url = \"https://archive.org/advancedsearch.php?q=collection:{id}&fl[]=identifier&rows=999999&output=json\"\n",
    "meta_url = \"https://archive.org/metadata/{id}\"\n",
    "file_url = \"https://archive.org/download/{id}/{file}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Set Collections\n",
    "\n",
    "You can load these from a text file (or just type them in here as a `list`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections = [\"hpjournal\"]\n",
    "\n",
    "#try:\n",
    "#    with open(collections_file, \"r\", encoding=\"utf-8\") as f:\n",
    "#        for line in f.readlines():\n",
    "#            collections.append(line.strip())\n",
    "#except FileNotFoundError:\n",
    "#    print(\"WRN: No collections file found.\")\n",
    "#    pass\n",
    "\n",
    "print(f\"Loaded {len(collections)} collections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Set Download Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_formats = [\"DjVuTXT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Set Download Date Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_date_range = (\"1975-01-01\", \"2005-12-31\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Find Items in Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "import requests\n",
    "\n",
    "collection_count = 0\n",
    "skip_count = 0\n",
    "item_count = 0\n",
    "\n",
    "\n",
    "# Create collections folder if it doesn't already exist.\n",
    "if not os.path.exists(metadata_path):\n",
    "    os.makedirs(os.path.abspath(metadata_path))\n",
    "\n",
    "\n",
    "for collection in collections:\n",
    "\n",
    "    collection_path = os.path.join(metadata_path, f\"{collection}.json\")\n",
    "\n",
    "    # Skip collection if we've already scraped it.\n",
    "    if os.path.exists(collection_path):\n",
    "        print(f\"Already scraped {collection}, skipping.\")\n",
    "        skip_count = skip_count + 1\n",
    "        continue\n",
    "\n",
    "    # Scrape a list of items from the collection.\n",
    "    print(f\"Getting items from {collection}...\")\n",
    "    collection_url = search_url.format(id=collection)\n",
    "    items = []\n",
    "    try:\n",
    "        r = requests.get(collection_url)\n",
    "        for item in r.json()[\"response\"][\"docs\"]:\n",
    "            items.append(item[\"identifier\"])\n",
    "            item_count = item_count + 1\n",
    "    except Exception:\n",
    "        print(f\"ERR: {collection_url}\")\n",
    "        skip_count = skip_count + 1\n",
    "        continue\n",
    "\n",
    "    # Skip the collection if we can't find the item metadata.\n",
    "    if len(items) == 0:\n",
    "        print(f\"WRN: No items found in {collection}!\")\n",
    "        skip_count = skip_count + 1\n",
    "        continue\n",
    "\n",
    "    # Save the collection to JSON.\n",
    "    collection_meta = {\n",
    "        \"name\": collection,\n",
    "        \"id\": str(uuid.uuid5(uuid.NAMESPACE_DNS, collection)),\n",
    "        \"url\": collection_url,\n",
    "        \"items\": items,\n",
    "    }\n",
    "    with open(collection_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(collection_meta, sort_keys=True, indent=4))\n",
    "    collection_count = collection_count + 1\n",
    "\n",
    "print(f\"\\n** DONE **\")\n",
    "print(f\"Found {item_count} new items in {collection_count} collections ({skip_count} skipped).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Get Item Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Load Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from time import sleep\n",
    "import uuid\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "# Load collections from JSON.\n",
    "# TODO: Catch JSON load errors.\n",
    "collections = []\n",
    "for file in [f for f in os.listdir(metadata_path) if f.endswith(\".json\")]:\n",
    "    with open(os.path.join(metadata_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "        collections.append(json.load(f))\n",
    "\n",
    "# Assemble item URLs to scrape.\n",
    "scrapes = []\n",
    "item_count = 0\n",
    "items_skipped = 0\n",
    "collections_skipped = 0\n",
    "\n",
    "for collection in collections:\n",
    "\n",
    "    items = []\n",
    "    for item in collection[\"items\"]:\n",
    "        if not isinstance(item, str):\n",
    "            items_skipped = items_skipped + 1\n",
    "            continue\n",
    "        items.append(item)\n",
    "        item_count = item_count + 1\n",
    "\n",
    "    if len(items) == 0:\n",
    "        collections_skipped = 0\n",
    "        continue\n",
    "\n",
    "    collection[\"items\"] = items\n",
    "    scrapes.append(collection)\n",
    "\n",
    "print(\n",
    "    f\"Found {item_count} items ({items_skipped} skipped) \" \\\n",
    "    f\"from {len(scrapes)} collections ({collections_skipped} skipped).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Get Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_count = 0\n",
    "skip_count = 0\n",
    "file_count = 0\n",
    "file_skip_count = 0\n",
    "\n",
    "for collection in scrapes:\n",
    "    for item in collection[\"items\"]:\n",
    "\n",
    "        print(f\"{collection['name']}/{item}\")\n",
    "        item_url = meta_url.format(id=item)\n",
    "        try:\n",
    "            r = requests.get(item_url)\n",
    "            data = r.json()\n",
    "        except Exception:\n",
    "            print(f\"ERR: {item_url}\")\n",
    "            skip_count = skip_count + 1\n",
    "            continue\n",
    "\n",
    "        # If there aren't any files we can move on.\n",
    "        if \"files\" not in data.keys() or data[\"files\"] == \"\" or len(data[\"files\"]) == 0:\n",
    "            print(f\"WRN: No items found in {collection['name']}!\")\n",
    "            skip_count = skip_count + 1\n",
    "            continue\n",
    "\n",
    "        # Store a list of files for scraping.\n",
    "        files = []\n",
    "        for file in [f for f in data[\"files\"] if f[\"name\"].endswith(\".txt\") or f[\"name\"].endswith(\".pdf\")]:\n",
    "            if file[\"format\"] == \"Metadata\":\n",
    "                file_skip_count = file_skip_count + 1\n",
    "                continue\n",
    "            files.append({\n",
    "                \"id\": str(uuid.uuid5(uuid.NAMESPACE_DNS, file[\"name\"])),\n",
    "                \"name\": file[\"name\"],\n",
    "                \"ext\": file[\"name\"][-3:],\n",
    "                \"format\": file[\"format\"],\n",
    "                \"url\": file_url.format(id=item, file=file[\"name\"]),\n",
    "                \"size\": file[\"size\"],\n",
    "                \"md5\": file[\"md5\"],\n",
    "                \"crc32\": file[\"crc32\"],\n",
    "                \"sha1\": file[\"sha1\"],\n",
    "            })\n",
    "            file_count = file_count + 1\n",
    "\n",
    "        # Grab the rest of the item metadata.\n",
    "        metadata = {\n",
    "            \"id\": str(uuid.uuid5(uuid.NAMESPACE_DNS, item)),\n",
    "            \"name\": item,\n",
    "            \"title\": data[\"metadata\"].get(\"title\", \"\"),\n",
    "            \"date\": data[\"metadata\"].get(\"date\", \"\"),\n",
    "            \"language\": data[\"metadata\"].get(\"language\", \"\"),\n",
    "            \"tags\": data[\"metadata\"].get(\"subject\", []),\n",
    "            \"files\": files,\n",
    "        }\n",
    "\n",
    "        # Save this all back to the file.\n",
    "        with open(os.path.join(metadata_path, f\"{collection['name']}.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "            collection_data = json.load(f)\n",
    "        for x in range(len(collection_data[\"items\"])):\n",
    "            if collection_data[\"items\"][x] == item:\n",
    "                collection_data[\"items\"][x] = metadata\n",
    "                break\n",
    "        with open(os.path.join(metadata_path, f\"{collection['name']}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(collection_data, sort_keys=True, indent=4))\n",
    "        item_count = item_count + 1\n",
    "\n",
    "print(f\"\\n** DONE! **\")\n",
    "print(\n",
    "    f\"Collected metadata for {item_count} items ({skip_count} skipped) \" \\\n",
    "    f\"and {file_count} files ({file_skip_count} skipped).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Find Files in Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import dateparser\n",
    "import regex as re\n",
    "\n",
    "re_date_full = re.compile(r\"(19[6-9]\\d|20[0-1]\\d)-[0-1]\\d\")\n",
    "re_date_year = re.compile(r\"19[6-9]\\d|20[0-1]\\d\")\n",
    "\n",
    "start_date = dateparser.parse(dl_date_range[0])\n",
    "end_date = dateparser.parse(dl_date_range[1])\n",
    "\n",
    "\n",
    "# Load collections from JSON.\n",
    "# TODO: Catch JSON load errors.\n",
    "collections = []\n",
    "for file in [f for f in os.listdir(metadata_path) if f.endswith(\".json\")]:\n",
    "    with open(os.path.join(metadata_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "        collections.append(json.load(f))\n",
    "\n",
    "# Assemble download URLs.\n",
    "downloads = []\n",
    "skip_count = 0\n",
    "\n",
    "for collection in collections:\n",
    "    for item in collection[\"items\"]:\n",
    "\n",
    "        # Check for metadata.\n",
    "        if isinstance(item, str):\n",
    "            print(f\"WRN: No metadata for {item['name']}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Find date.\n",
    "        date = item[\"date\"]\n",
    "        try:\n",
    "            date = dateparser.parse(date, settings=dict(REQUIRE_PARTS=[\"month\", \"year\"]))\n",
    "            if date < start_date or date > end_date:\n",
    "                skip_count = skip_count + len(item[\"files\"])\n",
    "                continue\n",
    "            date = date.strftime(\"%Y-%m\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        for file in item[\"files\"]:\n",
    "\n",
    "            # Check file type.\n",
    "            if file[\"format\"] not in dl_formats:\n",
    "                skip_count = skip_count + 1\n",
    "                continue\n",
    "\n",
    "            # Find date (if not stored in item metadata).\n",
    "            if not date or date == \"\":\n",
    "                try:\n",
    "                    date = re_date_full.search(file[\"name\"]).group(0)\n",
    "                    if date < start_date or date > end_date:\n",
    "                        skip_count = skip_count + 1\n",
    "                        continue\n",
    "                    date = date.strftime(\"%Y-%m\")\n",
    "                except Exception:\n",
    "                    skip_count = skip_count + 1\n",
    "                    continue\n",
    "\n",
    "            # Assemble a full filename.\n",
    "            # NB: a handful of files have local path names boiled in somehow. These tend to\n",
    "            # break things, so let's get rid of them.\n",
    "            filename = file[\"name\"][:-4].split(\"/\")[-1].split(\"\\\\\")[-1]\n",
    "            filename = f\"{date} - {filename} - {file['id']}.{file['ext']}\"\n",
    "\n",
    "\n",
    "            # Make sure we don't already have it.\n",
    "            if os.path.exists(os.path.join(download_path, filename)):\n",
    "                skip_count = skip_count + 1\n",
    "                continue\n",
    "\n",
    "            # Add it to the pile.\n",
    "            print(f\"Adding {file['url']}\")\n",
    "            downloads.append((file[\"url\"], filename))\n",
    "\n",
    "print(f\"\\n** DONE! **\")\n",
    "print(f\"Found {len(downloads)} files to download ({skip_count} skipped).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Download Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "\n",
    "skip_count = 0\n",
    "download_count = 0\n",
    "\n",
    "for url, filename in downloads:\n",
    "\n",
    "    if not os.path.exists(download_path):\n",
    "        os.makedirs(os.path.abspath(download_path))\n",
    "\n",
    "    try:\n",
    "        print(f\"Downloading {url} ...\")\n",
    "        newfile = wget.download(url)\n",
    "    except Exception:\n",
    "        print(f\"ERR: {url}\")\n",
    "        skip_count = skip_count + 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        os.rename(os.path.abspath(newfile), os.path.abspath(os.path.join(download_path, filename)))\n",
    "    except FileExistsError:\n",
    "        print(f\"ERR: {filename}\")\n",
    "        continue\n",
    "\n",
    "    download_count = download_count + 1\n",
    "\n",
    "\n",
    "print(f\"\\n** DONE! **\")\n",
    "print(f\"Downloaded {download_count} files ({skip_count} skipped).\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee8b9ecf46cc6e5ba1fee164dfe9a5d29badbac7e0088ca9fcb20f159d16cfe8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHOMP v2\n",
    "__Misc. Utilities__\n",
    "\n",
    "__by Kate Gilleran__  \n",
    "__Last updated November 30__, __2021__  \n",
    "[https://github.com/seangilleran/chomp2](https://github.com/seangilleran/chomp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Language (With Spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import en_core_web_sm\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "path = \"./corpus\"\n",
    "\n",
    "\n",
    "# Load language detector.\n",
    "@Language.factory(\"language_detector\")\n",
    "def language_detector(nlp, name):\n",
    "    return LanguageDetector()\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.add_pipe(\"language_detector\", last=True)\n",
    "\n",
    "for file in [f for f in os.listdir(path) if f.endswith(f\".txt\")]:\n",
    "\n",
    "    with open(os.path.join(path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Check language.\n",
    "    lang = nlp(text)._.language\n",
    "    language = lang[\"language\"]\n",
    "    score = lang[\"score\"]\n",
    "\n",
    "    print(f\"{language.capitalize()} ({(score * 100):.0f}%): {file}\")\n",
    "    with open(\"lang_check.csv\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"{language},{score},{file}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Language (With Tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "path = \"./meta\"\n",
    "\n",
    "for file in [f for f in os.listdir(path) if f.endswith(\".json\")]:\n",
    "\n",
    "    with open(os.path.join(path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "        collection = json.loads(f.read())\n",
    "\n",
    "    for item in collection[\"items\"]:\n",
    "        lang = item[\"language\"]\n",
    "\n",
    "        for file in item[\"files\"]:\n",
    "            with open(\"lang_check.csv\", \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"{lang},{file['name']},{file['id']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert PDF to TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Words vs. Enchant Spellcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import regex as re\n",
    "\n",
    "path = \".\\corpus\"\n",
    "files = []\n",
    "words = set()\n",
    "\n",
    "az = re.compile(r\"^[a-zA-Z]+$\")\n",
    "\n",
    "\n",
    "files = [f for f in os.listdir(path) if f.endswith(\".txt\")]\n",
    "\n",
    "print(f\"Loading {len(files)} files...\")\n",
    "\n",
    "for file in files:\n",
    "    word_count = len(words)\n",
    "    with open(os.path.join(path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    for word in word_tokenize(text):\n",
    "        if not az.search(word):\n",
    "            continue\n",
    "        words.add(word)\n",
    "    print(f\"Added {len(words) - word_count} words from {file}.\")\n",
    "\n",
    "print(\"\\n** DONE! **\")\n",
    "print(f\"Found {len(words)} unique words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "\n",
    "d = enchant.Dict(\"en_US\")\n",
    "\n",
    "\n",
    "words = list(words)\n",
    "words.sort()\n",
    "print(f\"Checking {len(words)} words...\")\n",
    "\n",
    "ok_count = 0\n",
    "nf_count = 0\n",
    "\n",
    "for word in words:\n",
    "    print(f\"(?) {word} ...\", end=\"\")\n",
    "    if not d.check(word):\n",
    "        with open(\"words.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"{word}\\n\")\n",
    "        print(\"Not Found\")\n",
    "        continue\n",
    "    print(\"OK\")\n",
    "\n",
    "total = ok_count + nf_count\n",
    "print(\"\\n** DONE! **\")\n",
    "print(f\"Could not find spelling for {nf_count} words out of {total} (corpus {(ok_count/total):.0f}% ok).\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7951fdec88d234d74f52ca0ecf0a11a9953c18796954f52a095ffbb55ed7d3e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
